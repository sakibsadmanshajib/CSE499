{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import warnings\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "import lightgbm as lgb\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "import gc\r\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv'))\r\n",
    "test = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\r\n",
    "labels = pd.read_csv(os.path.join(DATA_DIRECTORY, 'labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.to_numpy()\r\n",
    "test_id = test['SK_ID_CURR']\r\n",
    "train_id = train['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['SK_ID_CURR'], axis=1)\r\n",
    "test = test.drop(['SK_ID_CURR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median')\r\n",
    "imputer.fit(train)\r\n",
    "train = imputer.transform(train)\r\n",
    "test = imputer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, test_features, labels, test_ids, n_folds = 5):\r\n",
    "    \r\n",
    "    \r\n",
    "    cat_indices = 'auto'\r\n",
    "        \r\n",
    "    print('Training Data Shape: ', features.shape)\r\n",
    "    print('Testing Data Shape: ', test_features.shape)\r\n",
    "    \r\n",
    "    \r\n",
    "    # Convert to np arrays\r\n",
    "    features = np.array(features)\r\n",
    "    test_features = np.array(test_features)\r\n",
    "    \r\n",
    "    # Create the kfold object\r\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 88)\r\n",
    "    \r\n",
    "    # Empty array for test predictions\r\n",
    "    test_predictions = np.zeros(test_features.shape[0])\r\n",
    "    \r\n",
    "    # Empty array for out of fold validation predictions\r\n",
    "    out_of_fold = np.zeros(features.shape[0])\r\n",
    "    \r\n",
    "    # Lists for recording validation and training scores\r\n",
    "    valid_scores = []\r\n",
    "    train_scores = []\r\n",
    "    \r\n",
    "    # Iterate through each fold\r\n",
    "    for train_indices, valid_indices in k_fold.split(features):\r\n",
    "        \r\n",
    "        # Training data for the fold\r\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\r\n",
    "        # Validation data for the fold\r\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\r\n",
    "        \r\n",
    "        params = {'random_state': 88888, 'nthread': -1}\r\n",
    "        # Create the model\r\n",
    "        model = lgb.LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\r\n",
    "        \r\n",
    "        # Train the model\r\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\r\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\r\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\r\n",
    "                  early_stopping_rounds = 2500, verbose = 500)\r\n",
    "        \r\n",
    "        # Record the best iteration\r\n",
    "        best_iteration = model.best_iteration_\r\n",
    "        \r\n",
    "        # Make predictions\r\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\r\n",
    "        \r\n",
    "        # Record the out of fold predictions\r\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\r\n",
    "        \r\n",
    "        # Record the best score\r\n",
    "        valid_score = model.best_score_['valid']['auc']\r\n",
    "        train_score = model.best_score_['train']['auc']\r\n",
    "        \r\n",
    "        valid_scores.append(valid_score)\r\n",
    "        train_scores.append(train_score)\r\n",
    "        \r\n",
    "        # Clean up memory\r\n",
    "        gc.enable()\r\n",
    "        del model, train_features, valid_features\r\n",
    "        gc.collect()\r\n",
    "        \r\n",
    "    # Make the submission dataframe\r\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\r\n",
    "    \r\n",
    "    # Overall validation score\r\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\r\n",
    "    \r\n",
    "    # Add the overall scores to the metrics\r\n",
    "    valid_scores.append(valid_auc)\r\n",
    "    train_scores.append(np.mean(train_scores))\r\n",
    "    \r\n",
    "    # Needed for creating dataframe of validation scores\r\n",
    "    fold_names = list(range(n_folds))\r\n",
    "    fold_names.append('overall')\r\n",
    "    \r\n",
    "    # Dataframe of validation scores\r\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\r\n",
    "                            'train': train_scores,\r\n",
    "                            'valid': valid_scores}) \r\n",
    "    \r\n",
    "    return submission, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGHTGBM_PARAMS = {\r\n",
    "    'boosting_type': 'goss',\r\n",
    "    'n_estimators': 8888,\r\n",
    "    'learning_rate': 0.0098,\r\n",
    "    'num_leaves': 58,\r\n",
    "    'max_depth': 11,\r\n",
    "    'reg_alpha': 3.564,\r\n",
    "    'reg_lambda': 4.930,\r\n",
    "    'colsample_bytree': 0.613,\r\n",
    "    'subsample': 0.708,\r\n",
    "    'silent': -1,\r\n",
    "    'verbose': -1,\r\n",
    "    'max_bin': 407,\r\n",
    "    'min_child_weight': 6,\r\n",
    "    'min_child_samples': 165\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission, metrics = model(train, train, labels, train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission['TARGET'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(labels, submission)\r\n",
    "# Calculate the G-mean\r\n",
    "gmean = np.sqrt(tpr * (1 - fpr))\r\n",
    "\r\n",
    "# Find the optimal threshold\r\n",
    "index = np.argmax(gmean)\r\n",
    "bestThreshold = thresholds[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.where(submission > bestThreshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fpr, gmean, imputer, index, DATA_DIRECTORY, thresholds, tpr, submission, metrics, LIGHTGBM_PARAMS, bestThreshold\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\r\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, test_features, labels, test_ids, n_folds = 5):\r\n",
    "        \r\n",
    "    print('Training Data Shape: ', features.shape)\r\n",
    "    print('Testing Data Shape: ', test_features.shape)\r\n",
    "    \r\n",
    "    # Create the kfold object\r\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 88)\r\n",
    "    \r\n",
    "    # Empty array for test predictions\r\n",
    "    test_predictions = np.zeros(test_features.shape[0])\r\n",
    "    \r\n",
    "    # Empty array for out of fold validation predictions\r\n",
    "    out_of_fold = np.zeros(features.shape[0])\r\n",
    "    \r\n",
    "    # Lists for recording validation and training scores\r\n",
    "    valid_scores = []\r\n",
    "    train_scores = []\r\n",
    "\r\n",
    "    # Iterate through each fold\r\n",
    "    for train_indices, valid_indices in k_fold.split(features):\r\n",
    "        \r\n",
    "        # Training data for the fold\r\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\r\n",
    "        # Validation data for the fold\r\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\r\n",
    "\r\n",
    "        # Create the model\r\n",
    "        model = TabNetClassifier(\r\n",
    "                    n_d=32, \r\n",
    "                    n_a=32, \r\n",
    "                    n_steps=10,\r\n",
    "                    gamma=0.098, \r\n",
    "                    n_independent=2, \r\n",
    "                    n_shared=2,\r\n",
    "                    lambda_sparse=1e-3, \r\n",
    "                    momentum=0.4, \r\n",
    "                    clip_value=2.,\r\n",
    "                    optimizer_fn=torch.optim.Adam,\r\n",
    "                    scheduler_params = {\"gamma\": 0.95,\r\n",
    "                                    \"step_size\": 20},\r\n",
    "                    optimizer_params=dict(lr=2e-2),\r\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR, \r\n",
    "                    epsilon=1e-15, verbose = 0,\r\n",
    "                    device_name='cuda'\r\n",
    "                )\r\n",
    "        \r\n",
    "        # Train the model\r\n",
    "        model.fit(\r\n",
    "            train_features, train_labels,\r\n",
    "            eval_set=[(train_features, train_labels), (valid_features, valid_labels)],  \r\n",
    "            eval_name=['train', 'valid'],\r\n",
    "            eval_metric=['auc'],\r\n",
    "            max_epochs=1000 , patience=50,\r\n",
    "            batch_size=1024, virtual_batch_size=128,\r\n",
    "            num_workers=0,\r\n",
    "            weights=1,\r\n",
    "            drop_last=False\r\n",
    "        )\r\n",
    "\r\n",
    "        print(model)\r\n",
    "        \r\n",
    "        # Make predictions\r\n",
    "        test_predictions += model.predict_proba(test_features)[:, 1] / k_fold.n_splits\r\n",
    "        \r\n",
    "        # Record the out of fold predictions\r\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features)[:, 1]\r\n",
    "\r\n",
    "        # Record the best score\r\n",
    "        valid_score = roc_auc_score(valid_labels, model.predict(valid_features))\r\n",
    "        train_score = roc_auc_score(train_labels, model.predict(train_features))\r\n",
    "        \r\n",
    "        valid_scores.append(valid_score)\r\n",
    "        train_scores.append(train_score)\r\n",
    "        \r\n",
    "        # Clean up memory\r\n",
    "        gc.enable()\r\n",
    "        del model, train_features, valid_features\r\n",
    "        gc.collect()\r\n",
    "        \r\n",
    "    # Make the submission dataframe\r\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\r\n",
    "    \r\n",
    "    # Overall validation score\r\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\r\n",
    "    \r\n",
    "    # Add the overall scores to the metrics\r\n",
    "    valid_scores.append(valid_auc)\r\n",
    "    train_scores.append(np.mean(train_scores))\r\n",
    "    \r\n",
    "    # Needed for creating dataframe of validation scores\r\n",
    "    fold_names = list(range(n_folds))\r\n",
    "    fold_names.append('overall')\r\n",
    "    \r\n",
    "    # Dataframe of validation scores\r\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\r\n",
    "                            'train': train_scores,\r\n",
    "                            'valid': valid_scores}) \r\n",
    "    \r\n",
    "    return submission, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission, metrics = model(train, test, target, test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TabNet metrics')\r\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "scaler = MinMaxScaler()\r\n",
    "tn = submission['TARGET'].to_numpy().reshape(-1, 1)\r\n",
    "scaler.fit(tn)\r\n",
    "tg = scaler.transform(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'SK_ID_CURR': test_id, 'TARGET': tg[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('lightgbm-tabnet.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2acef9c405ed574e8ba965b223740abc6391b41de725cf0c976ada615306344"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('CSE499': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
