{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, merge, Reshape, Dropout, Input, Flatten, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "DATA_DIRECTORY = \"../input/home-credit-default-risk\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbab75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_roc_curve(y_, oof_preds_, folds_idx_):\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(6,6))\n",
    "    scores = [] \n",
    "    for n_fold, (_, val_idx) in enumerate(folds_idx_):  \n",
    "        # Plot the roc curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        score = roc_auc_score(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        scores.append(score)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\n",
    "    fpr, tpr, thresholds = roc_curve(y_, oof_preds_)\n",
    "    score = roc_auc_score(y_, oof_preds_)\n",
    "    plt.plot(fpr, tpr, color='b',\n",
    "             label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n",
    "             lw=2, alpha=.8)\n",
    "    \n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Embedding Neural Network ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('roc_curve.png')\n",
    "    \n",
    "def display_precision_recall(y_, oof_preds_, folds_idx_):\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    scores = [] \n",
    "    for n_fold, (_, val_idx) in enumerate(folds_idx_):  \n",
    "        # Plot the roc curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        score = average_precision_score(y_.iloc[val_idx], oof_preds_[val_idx])\n",
    "        scores.append(score)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='AP fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_, oof_preds_)\n",
    "    score = average_precision_score(y_, oof_preds_)\n",
    "    plt.plot(precision, recall, color='b',\n",
    "             label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n",
    "             lw=2, alpha=.8)\n",
    "    \n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Embedding Neural Network Recall / Precision')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('recall_precision_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_dir, debug=False):\n",
    "    # No target encoding\n",
    "    num_rows = 10000 if debug else None\n",
    "    \n",
    "    print('Preprocessing started.')\n",
    "    print('Bureau_Balance')\n",
    "    buro_bal = pd.read_csv(input_dir + 'bureau_balance.csv', nrows=num_rows)\n",
    "    \n",
    "    buro_counts = buro_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n",
    "    buro_bal['buro_count'] = buro_bal['SK_ID_BUREAU'].map(buro_counts['MONTHS_BALANCE'])\n",
    "    \n",
    "    avg_buro_bal = buro_bal.groupby('SK_ID_BUREAU').mean()\n",
    "    \n",
    "    avg_buro_bal.columns = ['avg_buro_' + f_ for f_ in avg_buro_bal.columns]\n",
    "    del buro_bal\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Bureau')\n",
    "    buro_full = pd.read_csv(input_dir + 'bureau.csv', nrows=num_rows)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    buro_full = buro_full.merge(right=avg_buro_bal.reset_index(), how='left', on='SK_ID_BUREAU', suffixes=('', '_bur_bal'))\n",
    "    \n",
    "    nb_bureau_per_curr = buro_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n",
    "    buro_full['SK_ID_BUREAU'] = buro_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n",
    "    \n",
    "    avg_buro = buro_full.groupby('SK_ID_CURR').mean()\n",
    "    \n",
    "    del buro_full\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Previous_Application')\n",
    "    prev = pd.read_csv(input_dir + 'previous_application.csv', nrows=num_rows)\n",
    "    \n",
    "    prev_cat_features = [\n",
    "        f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n",
    "    \n",
    "    avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "    del prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print('POS_CASH_Balance')\n",
    "    pos = pd.read_csv(input_dir + 'POS_CASH_balance.csv', nrows=num_rows)\n",
    "    \n",
    "    \n",
    "    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    avg_pos = pos.groupby('SK_ID_CURR').mean()\n",
    "    \n",
    "    del pos, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Credit_Card_Balance')\n",
    "    cc_bal = pd.read_csv(input_dir + 'credit_card_balance.csv', nrows=num_rows)\n",
    "\n",
    "    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n",
    "    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n",
    "    \n",
    "    del cc_bal, nb_prevs\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Installments_Payments')\n",
    "    inst = pd.read_csv(input_dir + 'installments_payments.csv', nrows=num_rows)\n",
    "    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "    \n",
    "    avg_inst = inst.groupby('SK_ID_CURR').mean()\n",
    "    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n",
    "    \n",
    "    print('Train/Test')\n",
    "    data = pd.read_csv(input_dir + 'application_train.csv', nrows=num_rows)\n",
    "    test = pd.read_csv(input_dir + 'application_test.csv', nrows=num_rows)\n",
    "    print('Shapes : ', data.shape, test.shape)\n",
    "        \n",
    "    data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    test = test.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del avg_buro, avg_prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Preprocessing done.')\n",
    "\n",
    "    return data, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preprocessing('../input/home-credit-default-risk/' , debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target and the ID\n",
    "X_train, y_train = train.iloc[:,2:], train.TARGET\n",
    "X_test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c950891",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vals_dict = {c: list(X_train[c].unique()) for c in X_train.columns if X_train[c].dtype == object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_numeric   = len(X_train.columns) - len(col_vals_dict)\n",
    "nb_categoric = len(col_vals_dict)\n",
    "print('Number of Numerical features:', nb_numeric)\n",
    "print('Number of Categorical features:', nb_categoric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9248bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the labels of each features\n",
    "col_vals_dict = {c: list(X_train[c].unique()) for c in X_train.columns if X_train[c].dtype == object}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to parse the cat\n",
    "generator = (c for c in X_train.columns if X_train[c].dtype == object)\n",
    "\n",
    "# Label Encoder\n",
    "for c in generator:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(X_train[c].values) + list(X_test[c].values))\n",
    "    X_train[c] = lbl.transform(list(X_train[c].values))\n",
    "    X_test[c] = lbl.transform(list(X_test[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols = []\n",
    "len_embed_cols = []\n",
    "for c in col_vals_dict:\n",
    "    if len(col_vals_dict[c])>2:\n",
    "        embed_cols.append(c)\n",
    "        len_embed_cols.append(len(col_vals_dict[c]))\n",
    "        print(c + ': %d values' % len(col_vals_dict[c])) #look at value counts to know the embedding dimensions\n",
    "        \n",
    "print('\\n Number of embed features :', len(embed_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a69e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_network(len_embed_cols):\n",
    "    \n",
    "    model_out = []\n",
    "    model_in  = []\n",
    "    \n",
    "    for dim in len_embed_cols:\n",
    "        input_dim = Input(shape=(1,), dtype='int32')\n",
    "        embed_dim = Embedding(dim, dim//2, input_length=1)(input_dim)\n",
    "        embed_dim = Dropout(0.25)(embed_dim)\n",
    "        embed_dim = Reshape((dim//2,))(embed_dim)\n",
    "        model_out.append(embed_dim)\n",
    "        model_in.append(input_dim)\n",
    "    \n",
    "    input_num = Input(shape=(176,), dtype='float32')\n",
    "    outputs = Concatenate(axis=1)([*model_out, input_num])\n",
    "    \n",
    "    outputs = (Dense(128))(outputs) \n",
    "    outputs = (Activation('relu'))(outputs)\n",
    "    outputs = (Dropout(.35))(outputs)\n",
    "    outputs = (Dense(64))(outputs)\n",
    "    outputs = (Activation('relu'))(outputs)\n",
    "    outputs = (Dropout(.15))(outputs)\n",
    "    outputs = (Dense(32))(outputs) \n",
    "    outputs = (Activation('relu'))(outputs)\n",
    "    outputs = (Dropout(.15))(outputs)\n",
    "    outputs = (Dense(1))(outputs)\n",
    "    outputs = (Activation('sigmoid'))(outputs)\n",
    "    \n",
    "    model = Model([*model_in, input_num], outputs)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X_train, X_val, X_test):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_val = []\n",
    "    input_list_test = []\n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(X_train[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(X_train[c].map(val_map).values)\n",
    "        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n",
    "        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n",
    "        \n",
    "    #the rest of the columns\n",
    "    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n",
    "    input_list_train.append(X_train[other_cols].values)\n",
    "    input_list_val.append(X_val[other_cols].values)\n",
    "    input_list_test.append(X_test[other_cols].values)\n",
    "    \n",
    "    return input_list_train, input_list_val, input_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9674c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train, X_train, X_test)\n",
    "print('Length of the list:', len(proc_X_train_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af245ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_X_train_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdbf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proc_X_train_f[12].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del proc_X_train_f, proc_X_val_f, proc_X_test_f\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [x for x in X_train.columns if x not in embed_cols]\n",
    "\n",
    "\n",
    "# Impute missing values in order to scale\n",
    "X_train[num_cols] = X_train[num_cols].fillna(value = 0)\n",
    "X_test[num_cols] = X_test[num_cols].fillna(value = 0)\n",
    "\n",
    "# Fit the scaler only on train data\n",
    "scaler = MinMaxScaler().fit(X_train[num_cols])\n",
    "X_train.loc[:,num_cols] = scaler.transform(X_train[num_cols])\n",
    "X_test.loc[:,num_cols] = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 7\n",
    "runs_per_fold = 2\n",
    "n_epochs = 150\n",
    "patience = 5\n",
    "\n",
    "cv_aucs   = []\n",
    "full_val_preds = np.zeros(np.shape(X_train)[0])\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K,  \n",
    "                            shuffle = True, random_state=1)\n",
    "\n",
    "for i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n",
    "    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n",
    "    \n",
    "    X_test_f = X_test.copy()\n",
    "    \n",
    "    \n",
    "    # Shuffle data\n",
    "    idx = np.arange(len(X_train_f))\n",
    "    np.random.shuffle(idx)\n",
    "    X_train_f = X_train_f.iloc[idx]\n",
    "    y_train_f = y_train_f.iloc[idx]\n",
    "    \n",
    "    #preprocessing\n",
    "    proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train_f, X_val_f, X_test_f)\n",
    "    \n",
    "    #track oof prediction for cv scores\n",
    "    val_preds = 0\n",
    "    \n",
    "    for j in range(runs_per_fold):\n",
    "    \n",
    "        NN = build_embedding_network(len_embed_cols)\n",
    "\n",
    "        # Set callback functions to early stop training and save the best model so far\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "        NN.fit(proc_X_train_f, y_train_f.values, epochs=n_epochs, batch_size=4096, verbose=1, callbacks=callbacks,validation_data=(proc_X_val_f, y_val_f))\n",
    "        \n",
    "        val_preds += NN.predict(proc_X_val_f)[:,0] / runs_per_fold\n",
    "        y_preds[:,i] += NN.predict(proc_X_test_f)[:,0] / runs_per_fold\n",
    "        \n",
    "    full_val_preds[outf_ind] += val_preds\n",
    "        \n",
    "    cv_auc  = roc_auc_score(y_val_f.values, val_preds)\n",
    "    cv_aucs.append(cv_auc)\n",
    "    print ('\\nFold %i prediction cv AUC: %.5f\\n' %(i,cv_auc))\n",
    "    \n",
    "print('Mean out of fold AUC: %.5f' % np.mean(cv_auc))\n",
    "print('Full validation AUC: %.5f' % roc_auc_score(y_train.values, full_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_idx = [(trn_idx, val_idx) for trn_idx, val_idx in kfold.split(X_train, y_train)]\n",
    "display_roc_curve(y_=y_train, oof_preds_=full_val_preds, folds_idx_=folds_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_precision_recall(y_=y_train, oof_preds_=full_val_preds, folds_idx_=folds_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccedbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['TARGET'] = np.mean(y_preds, axis=1)\n",
    "test = test[['SK_ID_CURR', 'TARGET']]\n",
    "out_df = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': test['TARGET']})\n",
    "out_df.to_csv('nn_embedding_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1362.507904,
   "end_time": "2021-09-15T21:19:09.928596",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-15T20:56:27.420692",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
