{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1faeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "nn_result=pd.read_csv('../input/sub-data/lgb.csv')\n",
    "log_result=pd.read_csv('../input/sub-data/nn_log.csv')\n",
    "nn_result.rename(columns={'TARGET':'nn_TARGET'},inplace=True)\n",
    "log_result.rename(columns={'TARGET':'log_TARGET'},inplace=True)\n",
    "# print(nn_result)\n",
    "# print(lgb_result)\n",
    "sub=pd.merge(nn_result,log_result,on='SK_ID_CURR')\n",
    "sub['TARGET']=0*sub['nn_TARGET']+1*sub['log_TARGET']\n",
    "sub=sub.drop(columns=['log_TARGET','nn_TARGET'])\n",
    "sub.to_csv('lgb_nn_log.csv', index=False, float_format='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f12dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "start_time = time.time()\n",
    "input_dir = os.path.join(os.pardir, 'input/sub-data')\n",
    "print('Loading data...')\n",
    "rows_read = None\n",
    "app_train_df = pd.read_csv(os.path.join(input_dir, 'lgb.csv'), nrows=rows_read)\n",
    "app_test_df = pd.read_csv(os.path.join(input_dir, 'lgb_nn_log.csv'), nrows=rows_read)\n",
    "print('Time elapsed %.0f sec'%(time.time()-start_time))\n",
    "print('Pre-processing data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets into a single one\n",
    "target = app_train_df.pop('TARGET')\n",
    "len_train = len(app_train_df)\n",
    "merged_df = pd.concat([app_train_df, app_test_df])\n",
    "meta_df = merged_df.pop('SK_ID_CURR')\n",
    "del app_test_df, app_train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals: 1-hot\n",
    "categorical_feats = merged_df.columns[merged_df.dtypes == 'object']\n",
    "print('Using %d prediction variables'%(merged_df.shape[1]))\n",
    "print('Encoding %d non-numeric columns...'%(merged_df.columns[merged_df.dtypes == 'object'].shape))\n",
    "for feat in categorical_feats:\n",
    "    merged_df[feat].fillna('MISSING', inplace=True) # populate missing labels\n",
    "    encoder = LabelBinarizer() # works with text\n",
    "    new_columns = encoder.fit_transform(merged_df[feat])\n",
    "    i=0\n",
    "    for u in merged_df[feat].unique():\n",
    "        if i<new_columns.shape[1]:\n",
    "            merged_df[feat+'_'+u]=new_columns[:,i]\n",
    "            i+=1\n",
    "    merged_df.drop(feat, axis=1, inplace=True)\n",
    "print('Now using %d prediction variables'%(merged_df.shape[1]))\n",
    "print('Time elapsed %.0f sec'%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "null_counts = merged_df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_ratios = null_counts / len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc345a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns over x% null\n",
    "null_thresh = .8\n",
    "null_cols = null_ratios[null_ratios > null_thresh].index\n",
    "merged_df.drop(null_cols, axis=1, inplace=True)\n",
    "if null_cols.shape[0] > 0:\n",
    "    print('Columns dropped for being over %.2f null:'%(null_thresh))\n",
    "    for col in null_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a89ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the rest with 0\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c556cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous features\n",
    "# first, convert large ingegers into floats.\n",
    "for feat in merged_df.columns:\n",
    "    if (merged_df[feat].max() > 100) | (merged_df[feat].min() < -100):\n",
    "        merged_df[feat]=merged_df[feat].astype(np.float64)\n",
    "scaler = StandardScaler()\n",
    "continuous_feats = merged_df.columns[merged_df.dtypes == 'float64']\n",
    "print('Scaling %d features...'%(continuous_feats.shape))\n",
    "s1 = merged_df.shape[0],1\n",
    "for feat in continuous_feats:\n",
    "    merged_df[feat] = scaler.fit_transform(merged_df[feat].values.reshape(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate into train and test\n",
    "train_df = merged_df[:len_train]\n",
    "test_df = merged_df[len_train:]\n",
    "del merged_df\n",
    "gc.collect()\n",
    "\n",
    "print('Time elapsed %.0f sec'%(time.time()-start_time))\n",
    "print('Starting training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201532b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train parameters\n",
    "L2c = 4e-4                    # loss, with L2\n",
    "lr0 = 0.02                    # starting learning rate\n",
    "lr_decay = 0.90               # lr decay rate\n",
    "iterations = 101               # full passes over data\n",
    "ROWS = train_df.shape[0]      # rows in input data\n",
    "VARS = train_df.shape[1]      # vars used in the model\n",
    "NUMB = 10000                  # batch size\n",
    "NN = int(ROWS/NUMB)           # number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "x  = tf.placeholder(tf.float32, [None, VARS])\n",
    "\n",
    "# model: logistic + 1 hidden layer\n",
    "W      = tf.Variable(tf.truncated_normal([VARS,1],mean=0.0,stddev=0.001),dtype=np.float32)\n",
    "NUML1  = 64\n",
    "W1     = tf.Variable(tf.truncated_normal([VARS,NUML1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "W1f    = tf.Variable(tf.truncated_normal([NUML1,1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "logit1 = tf.matmul( x, W ) + tf.matmul(tf.nn.relu(tf.matmul( x, W1 )), W1f)\n",
    "y      = tf.nn.sigmoid( logit1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss/optimizer\n",
    "loss0 = tf.reduce_mean( (y_-y)*(y_-y) )\n",
    "loss1 = L2c * (tf.nn.l2_loss( W ) + tf.nn.l2_loss( W1 ) + tf.nn.l2_loss( W1f ))\n",
    "loss  = loss0 + loss1\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr0, global_step, NN, lr_decay)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ca4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "y0=target.values.astype(np.float32)\n",
    "x0=train_df.values.astype(np.float32)\n",
    "del train_df\n",
    "gc.collect()\n",
    "y0_1=np.where(y0[0:int(NN*0.8)*NUMB] == 1)[0] # reserve last 20% for testing\n",
    "y0_0=np.where(y0[0:int(NN*0.8)*NUMB] == 0)[0] # reserve last 20% for testing\n",
    "for i in range(iterations):\n",
    "    for j in range(int(NN*0.8)): # reserve last 20% for testing\n",
    "        pos_ratio = 0.5\n",
    "        #pos_idx = np.random.choice(y0_1, size=int(np.round(NUMB*pos_ratio)))\n",
    "        #neg_idx = np.random.choice(y0_0, size=int(np.round(NUMB*(1-pos_ratio))))\n",
    "        #idx = np.concatenate([pos_idx, neg_idx])\n",
    "        #fd = {y_: y0[idx].reshape(NUMB,1),x:  x0[idx,:]}\n",
    "        #_= sess.run( [train_step], feed_dict=fd )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set and create submission\n",
    "x0     = test_df.values.astype(np.float32)\n",
    "fd     = {y_: np.zeros([x0.shape[0],1]),x: x0}\n",
    "y_pred = sess.run( y, feed_dict=fd )\n",
    "out_df = pd.DataFrame({'SK_ID_CURR': meta_df[len_train:], 'TARGET': y_pred[:,0]})\n",
    "out_df.to_csv('submission.csv', index=False)\n",
    "print('Time elapsed %.0f sec'%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08098278",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_result=pd.read_csv('../input/sub-data/lgb.csv')\n",
    "log_result=pd.read_csv('../input/sub-data/submission.csv')\n",
    "nn_result.rename(columns={'TARGET':'nn_TARGET'},inplace=True)\n",
    "log_result.rename(columns={'TARGET':'log_TARGET'},inplace=True)\n",
    "# print(nn_result)\n",
    "# print(lgb_result)\n",
    "sub=pd.merge(nn_result,log_result,on='SK_ID_CURR')\n",
    "sub['TARGET']=0.1*sub['nn_TARGET']+0.9*sub['log_TARGET']\n",
    "sub=sub.drop(columns=['log_TARGET','nn_TARGET'])\n",
    "sub.to_csv('9sub_lgb_nn_log.csv', index=False, float_format='%.8f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.882803,
   "end_time": "2021-07-25T09:19:54.579363",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-25T09:19:34.696560",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
