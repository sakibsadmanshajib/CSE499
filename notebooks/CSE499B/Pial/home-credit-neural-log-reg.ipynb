{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../input/home-credit-default-risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_DIRECTORY, 'application_train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(DATA_DIRECTORY, 'application_test.csv'))\n",
    "df = df_train.append(df_test)\n",
    "#len_train = len(df)\n",
    "del df_train, df_test; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['AMT_INCOME_TOTAL'] < 20000000]\n",
    "df = df[df['CODE_GENDER'] != 'XNA']\n",
    "df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_group(days_birth):\n",
    "    age_years = -days_birth / 365\n",
    "    if age_years < 27: return 1\n",
    "    elif age_years < 40: return 2\n",
    "    elif age_years < 50: return 3\n",
    "    elif age_years < 65: return 4\n",
    "    elif age_years < 99: return 5\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_group(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "    feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "    df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "        df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']    \n",
    "df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mean(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_median(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_std(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sum(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
    "df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
    "df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
    "df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
    "df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
    "df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
    "df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
    "df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
    "df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
    "df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df, categorical_columns=None):\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in categorical_columns:\n",
    "        df[col], uniques = pd.factorize(df[col])\n",
    "    return df, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_application_columns(df):\n",
    "    drop_list = [\n",
    "        'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n",
    "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
    "        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n",
    "        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n",
    "        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n",
    "        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n",
    "        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n",
    "        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n",
    "        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n",
    "        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n",
    "        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n",
    "        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n",
    "        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'\n",
    "    ]\n",
    "    for doc_num in [2,4,5,6,7,9,10,11,12,13,14,15,16,17,19,20,21]:\n",
    "        drop_list.append('FLAG_DOCUMENT_{}'.format(doc_num))\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, le_encoded_cols = label_encoder(df, None)\n",
    "df = drop_application_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-compression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-silicon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-fireplace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-burns",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-electricity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = pd.read_csv(os.path.join(DATA_DIRECTORY, 'bureau.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bureau_balance(path, num_rows= None):\n",
    "    bb = pd.read_csv(os.path.join(path, 'bureau_balance.csv'))\n",
    "    bb, categorical_cols = one_hot_encoder(bb, nan_as_category= False)\n",
    "    # Calculate rate for each category with decay\n",
    "    bb_processed = bb.groupby('SK_ID_BUREAU')[categorical_cols].mean().reset_index()\n",
    "    # Min, Max, Count and mean duration of payments (months)\n",
    "    agg = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'size']}\n",
    "    bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
    "    del bb; gc.collect()\n",
    "    return bb_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau, categorical_cols = one_hot_encoder(bureau, nan_as_category= False)\n",
    "bureau = bureau.merge(get_bureau_balance(DATA_DIRECTORY), how='left', on='SK_ID_BUREAU')\n",
    "bureau['STATUS_12345'] = 0\n",
    "for i in range(1,6):\n",
    "    bureau['STATUS_12345'] += bureau['STATUS_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM',\n",
    "    'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
    "agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n",
    "agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n",
    "bureau = bureau.merge(agg_length, how='left', on='MONTHS_BALANCE_SIZE')\n",
    "del agg_length; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUREAU_AGG = {\n",
    "    'SK_ID_BUREAU': ['nunique'],\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_1': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "    'STATUS_C': ['mean'],\n",
    "    'STATUS_X': ['mean'],\n",
    "    'CREDIT_ACTIVE_Active': ['mean'],\n",
    "    'CREDIT_ACTIVE_Closed': ['mean'],\n",
    "    'CREDIT_ACTIVE_Sold': ['mean'],\n",
    "    'CREDIT_TYPE_Consumer credit': ['mean'],\n",
    "    'CREDIT_TYPE_Credit card': ['mean'],\n",
    "    'CREDIT_TYPE_Car loan': ['mean'],\n",
    "    'CREDIT_TYPE_Mortgage': ['mean'],\n",
    "    'CREDIT_TYPE_Microloan': ['mean'],\n",
    "    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'LL_DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'LL_STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_ACTIVE_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_UPDATE': ['min', 'mean'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "}\n",
    "\n",
    "BUREAU_CLOSED_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'var'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n",
    "    'DAYS_CREDIT_UPDATE': ['max'],\n",
    "    'ENDDATE_DIF': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_LOAN_TYPE_AGG = {\n",
    "    'DAYS_CREDIT': ['mean', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'max'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "}\n",
    "\n",
    "BUREAU_TIME_AGG = {\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_bureau = group(bureau, 'BUREAU_', BUREAU_AGG)\n",
    "active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "agg_bureau = group_and_merge(active,agg_bureau,'BUREAU_ACTIVE_',BUREAU_ACTIVE_AGG)\n",
    "closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "agg_bureau = group_and_merge(closed,agg_bureau,'BUREAU_CLOSED_',BUREAU_CLOSED_AGG)\n",
    "del active, closed; gc.collect()\n",
    "for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n",
    "    type_df = bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n",
    "    prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n",
    "    agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_LOAN_TYPE_AGG)\n",
    "    del type_df; gc.collect()\n",
    "for time_frame in [6, 12]:\n",
    "    prefix = \"BUREAU_LAST{}M_\".format(time_frame)\n",
    "    time_frame_df = bureau[bureau['DAYS_CREDIT'] >= -30*time_frame]\n",
    "    agg_bureau = group_and_merge(time_frame_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n",
    "    del time_frame_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_bureau = bureau.sort_values(by=['DAYS_CREDIT'])\n",
    "gr = sort_bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].last().reset_index()\n",
    "gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)\n",
    "agg_bureau = agg_bureau.merge(gr, on='SK_ID_CURR', how='left')\n",
    "agg_bureau['BUREAU_DEBT_OVER_CREDIT'] = \\\n",
    "    agg_bureau['BUREAU_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_AMT_CREDIT_SUM_SUM']\n",
    "agg_bureau['BUREAU_ACTIVE_DEBT_OVER_CREDIT'] = \\\n",
    "    agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, agg_bureau, on='SK_ID_CURR', how='left')\n",
    "del agg_bureau, bureau\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = pd.read_csv(os.path.join(DATA_DIRECTORY, 'previous_application.csv'))\n",
    "pay = pd.read_csv(os.path.join(DATA_DIRECTORY, 'installments_payments.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREVIOUS_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['max'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n",
    "    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_ACTIVE_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'SIMPLE_INTERESTS': ['mean'],\n",
    "    'AMT_ANNUITY': ['max', 'sum'],\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'AMT_PAYMENT': ['sum'],\n",
    "    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n",
    "    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n",
    "    'REPAYMENT_RATIO': ['mean'],\n",
    "}\n",
    "PREVIOUS_LATE_PAYMENTS_AGG = {\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "PREVIOUS_LOAN_TYPE_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['max'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_TIME_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['mean', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_APPROVED_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max'],\n",
    "    'AMT_GOODS_PRICE': ['max'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['mean'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['max'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    # The following features are only for approved applications\n",
    "    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n",
    "    'DAYS_FIRST_DUE': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE': ['max', 'mean'],\n",
    "    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_REFUSED_AGG = {\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = [\n",
    "    'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n",
    "    'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
    "    'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n",
    "prev, categorical_cols = one_hot_encoder(prev, ohe_columns, nan_as_category= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT']/prev['AMT_ANNUITY']\n",
    "prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "prev['SIMPLE_INTERESTS'] = (total_payment/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n",
    "active_pay = pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]\n",
    "active_pay_agg = active_pay.groupby('SK_ID_PREV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n",
    "active_pay_agg.reset_index(inplace= True)\n",
    "active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - active_pay_agg['AMT_PAYMENT']\n",
    "active_df = active_df.merge(active_pay_agg, on= 'SK_ID_PREV', how= 'left')\n",
    "active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - active_df['AMT_PAYMENT']\n",
    "active_df['REPAYMENT_RATIO'] = active_df['AMT_PAYMENT'] / active_df['AMT_CREDIT']\n",
    "active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n",
    "active_agg_df['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREV_ACTIVE_AMT_PAYMENT_SUM']/\\\n",
    "                                            active_agg_df['PREV_ACTIVE_AMT_CREDIT_SUM']\n",
    "del active_pay, active_pay_agg, active_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "approved['DAYS_LAST_DUE_DIFF'] = approved['DAYS_LAST_DUE_1ST_VERSION'] - approved['DAYS_LAST_DUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_agg = {key: ['mean'] for key in categorical_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})\n",
    "agg_prev = agg_prev.merge(active_agg_df, how='left', on='SK_ID_CURR')\n",
    "del active_agg_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_prev = group_and_merge(approved, agg_prev, 'APPROVED_', PREVIOUS_APPROVED_AGG)\n",
    "refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "agg_prev = group_and_merge(refused, agg_prev, 'REFUSED_', PREVIOUS_REFUSED_AGG)\n",
    "del approved, refused; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loan_type in ['Consumer loans', 'Cash loans']:\n",
    "    type_df = prev[prev['NAME_CONTRACT_TYPE_{}'.format(loan_type)] == 1]\n",
    "    prefix = 'PREV_' + loan_type.split(\" \")[0] + '_'\n",
    "    agg_prev = group_and_merge(type_df, agg_prev, prefix, PREVIOUS_LOAN_TYPE_AGG)\n",
    "    del type_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay['LATE_PAYMENT'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n",
    "pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(lambda x: 1 if x > 0 else 0)\n",
    "dpd_id = pay[pay['LATE_PAYMENT'] > 0]['SK_ID_PREV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dpd = group_and_merge(prev[prev['SK_ID_PREV'].isin(dpd_id)], agg_prev,\n",
    "                                    'PREV_LATE_', PREVIOUS_LATE_PAYMENTS_AGG)\n",
    "del agg_dpd, dpd_id; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-entrance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_frame in [12, 24]:\n",
    "    time_frame_df = prev[prev['DAYS_DECISION'] >= -30*time_frame]\n",
    "    prefix = 'PREV_LAST{}M_'.format(time_frame)\n",
    "    agg_prev = group_and_merge(time_frame_df, agg_prev, prefix, PREVIOUS_TIME_AGG)\n",
    "    del time_frame_df; gc.collect()\n",
    "del prev; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, agg_prev, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['TARGET'].notnull()]\n",
    "test = df[df['TARGET'].isnull()]\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['TARGET']\n",
    "train = train.drop(columns=['TARGET'])\n",
    "test = test.drop(columns=['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = list(train.columns)\n",
    "\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_df = test.copy()\n",
    "train_df = train.copy()\n",
    "train_df['TARGET'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = SimpleImputer(strategy = 'median')\n",
    "#imputer.fit(train)\n",
    "#train = imputer.transform(train)\n",
    "#test = imputer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "#scaler.fit(train)\n",
    "#train = scaler.transform(train)\n",
    "#est = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_reg = LogisticRegression(C = 0.0001)\n",
    "#log_reg.fit(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_reg_pred = log_reg.predict_proba(test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit = test_df[['SK_ID_CURR']]\n",
    "#submit['TARGET'] = log_reg_pred\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets into a single one\n",
    "target = train_df.pop('TARGET')\n",
    "len_train = len(train_df)\n",
    "merged_df = pd.concat([train_df, test_df])\n",
    "meta_df = merged_df.pop('SK_ID_CURR')\n",
    "del test_df, train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-universal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals: 1-hot\n",
    "categorical_feats = merged_df.columns[merged_df.dtypes == 'object']\n",
    "print('Using %d prediction variables'%(merged_df.shape[1]))\n",
    "print('Encoding %d non-numeric columns...'%(merged_df.columns[merged_df.dtypes == 'object'].shape))\n",
    "for feat in categorical_feats:\n",
    "    merged_df[feat].fillna('MISSING', inplace=True) # populate missing labels\n",
    "    encoder = LabelBinarizer() # works with text\n",
    "    new_columns = encoder.fit_transform(merged_df[feat])\n",
    "    i=0\n",
    "    for u in merged_df[feat].unique():\n",
    "        if i<new_columns.shape[1]:\n",
    "            merged_df[feat+'_'+u]=new_columns[:,i]\n",
    "            i+=1\n",
    "    merged_df.drop(feat, axis=1, inplace=True)\n",
    "print('Now using %d prediction variables'%(merged_df.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "null_counts = merged_df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_ratios = null_counts / len(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns over x% null\n",
    "null_thresh = .8\n",
    "null_cols = null_ratios[null_ratios > null_thresh].index\n",
    "merged_df.drop(null_cols, axis=1, inplace=True)\n",
    "if null_cols.shape[0] > 0:\n",
    "    print('Columns dropped for being over %.2f null:'%(null_thresh))\n",
    "    for col in null_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the rest with 0\n",
    "merged_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous features\n",
    "# first, convert large ingegers into floats.\n",
    "for feat in merged_df.columns:\n",
    "    if (merged_df[feat].max() > 100) | (merged_df[feat].min() < -100):\n",
    "        merged_df[feat]=merged_df[feat].astype(np.float64)\n",
    "scaler = StandardScaler()\n",
    "continuous_feats = merged_df.columns[merged_df.dtypes == 'float64']\n",
    "print('Scaling %d features...'%(continuous_feats.shape))\n",
    "s1 = merged_df.shape[0],1\n",
    "for feat in continuous_feats:\n",
    "    merged_df[feat] = scaler.fit_transform(merged_df[feat].values.reshape(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate into train and test\n",
    "train_df = merged_df[:len_train]\n",
    "test_df = merged_df[len_train:]\n",
    "del merged_df\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train parameters\n",
    "L2c = 4e-4                    # loss, with L2\n",
    "lr0 = 0.02                    # starting learning rate\n",
    "lr_decay = 0.90               # lr decay rate\n",
    "iterations = 10               # full passes over data\n",
    "ROWS = train_df.shape[0]      # rows in input data\n",
    "VARS = train_df.shape[1]      # vars used in the model\n",
    "NUMB = 10000                  # batch size\n",
    "NN = int(ROWS/NUMB)           # number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "x  = tf.placeholder(tf.float32, [None, VARS])\n",
    "\n",
    "# model: logistic + 1 hidden layer\n",
    "W      = tf.Variable(tf.truncated_normal([VARS,1],mean=0.0,stddev=0.001),dtype=np.float32)\n",
    "NUML1  = 151\n",
    "W1     = tf.Variable(tf.truncated_normal([VARS,NUML1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "W1f    = tf.Variable(tf.truncated_normal([NUML1,1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "logit1 = tf.matmul( x, W ) + tf.matmul(tf.nn.relu(tf.matmul( x, W1 )), W1f)\n",
    "y      = tf.nn.sigmoid( logit1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss/optimizer\n",
    "loss0 = tf.reduce_mean( (y_-y)*(y_-y) )\n",
    "loss1 = L2c * (tf.nn.l2_loss( W ) + tf.nn.l2_loss( W1 ) + tf.nn.l2_loss( W1f ))\n",
    "loss  = loss0 + loss1\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr0, global_step, NN, lr_decay)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "y0=target.values.astype(np.float32)\n",
    "x0=train_df.values.astype(np.float32)\n",
    "del train_df\n",
    "gc.collect()\n",
    "y0_1=np.where(y0[0:int(NN*0.8)*NUMB] == 1)[0] # reserve last 20% for testing\n",
    "y0_0=np.where(y0[0:int(NN*0.8)*NUMB] == 0)[0] # reserve last 20% for testing\n",
    "for i in range(iterations):\n",
    "    for j in range(int(NN*0.8)): # reserve last 20% for testing\n",
    "        pos_ratio = 0.5\n",
    "        pos_idx = np.random.choice(y0_1, size=int(np.round(NUMB*pos_ratio)))\n",
    "        neg_idx = np.random.choice(y0_0, size=int(np.round(NUMB*(1-pos_ratio))))\n",
    "        idx = np.concatenate([pos_idx, neg_idx])\n",
    "        fd = {y_: y0[idx].reshape(NUMB,1),x:  x0[idx,:]}\n",
    "        _= sess.run( [train_step], feed_dict=fd )\n",
    "    if i%10 == 0:\n",
    "        # get area under the ROC curve\n",
    "        fd   = {y_: y0.reshape(y0.shape[0],1),x: x0}\n",
    "        y1   = sess.run( y, feed_dict=fd )\n",
    "        lim  = int(NN*0.8) * NUMB\n",
    "        auc1 = roc_auc_score(y0[0:lim],y1[0:lim,0])\n",
    "        auc2 = roc_auc_score(y0[lim:y0.shape[0]],y1[lim:y0.shape[0],0])\n",
    "        print('iteration %d, auc train/validatet %.5f/%.5f'%(i,auc1,auc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set and create submission\n",
    "x0     = test_df.values.astype(np.float32)\n",
    "fd     = {y_: np.zeros([x0.shape[0],1]),x: x0}\n",
    "y_pred = sess.run( y, feed_dict=fd )\n",
    "out_df = pd.DataFrame({'SK_ID_CURR': meta_df[len_train:], 'TARGET': y_pred[:,0]})\n",
    "out_df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 230.186338,
   "end_time": "2021-07-08T12:53:05.758316",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-08T12:49:15.571978",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
